MIRI

MACH IN E INT ELLIGENCE
R ESEARCH INS TITU TE

Convergence of Expected Utility
for Universal AI
Peter de Blanc
MIRI Visiting Fellow

Abstract
We consider a sequence of repeated interactions between an agent and an environment.
Uncertainty about the environment is captured by a probability distribution over a space
of hypotheses, which includes all computable functions. Given a utility function, we can
evaluate the expected utility of any computational policy for interaction with the environment. After making some plausible assumptions (and maybe one not-so-plausible
assumption), we show that if the utility function is unbounded, then the expected utility
of any policy is undefined.

de Blanc, Peter. 2009. Convergence of Expected Utility for Universal AI.
The Singularity Institute, San Francisco, CA.
The Machine Intelligence Research Institute was previously known as the Singularity Institute.

1. AI Formalism
We will assume that the interaction between the agent and the environment takes place
in discrete time-steps, or cycles. In cycle n, the agent outputs an action yn ∈ Y , and the
environment inputs to the agent a perception xn ∈ X. Y and X are the sets of possible
actions and perceptions, respectively, and are considered as subsets of N. Thus the story
of all interaction between agent and environment is captured by the two sequences x =
(x1 , x2 , . . . ) and y = (y1 , y2 , . . . ).
Let us introduce a notation for substrings. If s is a sequence or string, and {a, b} ⊆ N,
a ≤ b, then define sb = (sa , sa+1 , . . . sb ).
a
We will denote the function instantiated by the environment as Q : Y ∗ → X, so that
n
∀n ∈ N, xn = Q(y1 ). This means that the perception generated by the environment at

any given cycle is determined by the agent’s actions on that and all previous cycles.
A policy for the agent is a function p : (Y ∗ × X ∗ ) → Y , so that an agent implen−1
menting p at time n will choose an action yn = p y1 , xn−1 .
1

If, at any time, an agent adopts some policy p, and continues to follow that policy
forever, then p and Q taken together completely determine the future of the sequences
(xn ) and (yn ). We are particularly interested in the future sequence of perceptions, so
n
we will define a future function Ψ (Q, p, y1 , xn ) = x∞ .
1
n+1

Because the precise nature of the environment Q is unknown to the agent, we will let
Ω be the set of possible environments. Let F be a σ-algebra on Ω, and P : F → [0, 1]
be a probability measure on F which represents the agent’s prior information about the
environment.
We will also define a function Γq : Y ∗ → X ∗ which represents the perception string
output by environment q given some action string. Let
Γq (s) = (q (s1 ) , q (s2 ) , . . . , q (s)).
1
1
The agent will compare the quality of diﬀerent outcomes using a utility function
U : X N → R. We can then judge a policy by calculating the expected utility of the
outcome given that policy, which can be written as
n
n
E (U (xn Ψ (Q, p, y1 , xn )) |ΓQ (y1 ) = xn )
1
1
1

(1)

where Q is being treated as a random variable. When we write a string next to a sen
quence, as in xn Ψ (Q, p, y1 , xn ), we mean to concatenate them. Here, xn represents
1
1
1
n
what the agent has seen in the past, and Ψ (Q, p, y1 , xn ) represents something the agent
1

may see in the future. By concatenating them, we get a complete sequence of perceptions, which is the input required by the utility function U .
Notice that the expected utility above is a conditional expectation. Except on the
very first time-step, the agent will already have some knowledge about the environment.

n
After n cycles, the agent has output the string y1 , and the environment has output the
n
string xn . Thus the agent’s knowledge is given by the equation ΓQ (y1 ) = xn .
1
1
Agents such as AIXI (Hutter 2007) choose actions by comparing the expected utility

of diﬀerent policies. Thus we will focus, in this paper, on calculating the expected utility
of a policy.

2. Assumptions about the Hypothesis Space
Here we’ll make further assumptions about the hypothesis space (Ω, F, P ). While we
could succinctly make strong assumptions that would justify our central claim, we will
instead try to give somewhat weaker assumptions, even at the loss of some brevity.
Let ΩC be the set of computable total functions mapping Y ∗ to X. We will assume
that Ω ⊇ ΩC and that (∀q ∈ ΩC ) : {q} ∈ F and P ({q}) > 0. Thus we assume that the
agent assigns a nonzero probability to any computable environment function.
Let ΩP be the set of computable partial functions from Y ∗ to X. Then ΩC ⊂ ΩP . The
computable partial functions ΩP can be indexed by natural numbers, using a surjective
computable index function φ : N → ΩP . Since the codomain of φ is a set of partial
functions, it may be unclear what we mean when we say that φ is computable. We mean
that (i, s) → (φ (i)) (s), whose codomain is X, is a computable partial function. We
will also use the notation φi = φ (i).
We’ll now assume that there exists a computable total function ρ : N → Q such
that if φi ∈ ΩC , then 0 < ρ (i) ≤ P ({φ (i)}). Intuitively, we are saying that φ is a
way of describing computable functions using some sort of language, and that ρ is a way
of specifying lower bounds on probabilities based on these descriptions. Note that we
make no assumption about ρ (i) when φi ∈ ΩC .
/
To see an example of a hypothesis space satisfying all of our assumptions, let Ω = ΩC ,
let F = 2ΩP , let φ be any programming language, and let ρ (i) = 2−i . Let
O=

ρ (i)

(2)

i (φi ∈ΩC )

and for any ω ∈ Ω, let
P ({ω}) =

1
O

ρ (i)

(3)

i (φi =ω)

3. Assumptions about the Utility Function
Perhaps the most philosophically questionable assumption in this paper has already been
made in defining the domain of the utility function U as X N , the set of perception-

sequences. This is like assuming that a person cares not about his or her family and
friends, but about his or her perception of his or her family and friends.
Since the utility function U : X N → R takes as its argument an infinite sequence,
we must discuss what it means for such a function to be computable. Obviously any
computation which terminates can only look at a finite number of terms. Therefore we
will try to approximate U (x) using prefixes of x. We say that U is computable if there
exist computable functions UL , UU : X ∗ → Q ∪ {−∞, +∞} such that, if x ∈ X N and
x ∈ X ∗ and x
¯
¯

x, then:

• UL (¯) ≤ U (x) ≤ UU (¯)
x
x
• UL (¯) → U (x) and UU (¯) → U (x) as x → x.
x
x
¯
In any case, we will not assume that U is computable, because we do not need such a
strong assumption to prove our claims. Instead we will define two possible conditions.
Definition 1. Let D ⊆ X N and let U : D → R. Let Dp = {s ∈ X ∗ | (∃d ∈ D) :
s

d}. Then U is computably unbounded from above on D if there exists a com-

putable partial function UL : Dp → Z such that:
• (∀d ∈ D) (∀s ∈ Dp ) : if s

d, and if UL (s) exists, then UL (s) ≤ U (d).

• (∀m ∈ Z) (∃s ∈ Dp ) : UL (s) > m.
U is computably unbounded from below if −U is computably unbounded from above.
Note in particular that any computable function on X N which is unbounded from
above is computably unbounded from above, and any computable function which is
unbounded from below is computably unbounded from below.
The following lemma will help us find environments which generate large amounts
of utility. When considering f in the lemma, think of UL above.
Lemma 1. Suppose C ⊆ X ∗ , and f : C → Z is a computable partial function such
that (∀m ∈ Z) (∃c ∈ C) : f (c) > m. Then there exists a computable total function
H : Z → C such that, (∀m ∈ Z) : f ◦ H (m) ≥ m.
In other words, given an unbounded partial function f , there is a computable function
H which finds an input on which f will exceed any given bound.
Proof: First we’ll index C; let C = {c1 , c2 , . . . }.
If f were a total function, we could simply let H (m) = cmin{i∈N:f (ci )>m} . We would
compute this by first computing f (c1 ), then f (c2 ), etc. Unfortunately we only have that
f is a partial function, so we can not proceed in this way.
Instead, we’ll note that for any input on which f halts, it must halt in a specific
number of steps. The Cantor pairing function π : N × N → N, π (k1 , k2 ) = 1 (k1 +
2

k2 )(k1 + k2 + 1) + k2 is a bijection, so we can use π −1 to index all pairs of natural
numbers. Then we can simulate f on every possible input for every number of steps,
which will allow us to evaluate f on every input for which f halts.
def H(m):
for n in (1, 2, 3, ...):
let (t, i) = pi^-1(n)
simulate f(c_i) for t steps
... if it does not finish:
do nothing.
... if it does finish:
if f(c_i) >= m:
return c_i
Then H is a computable total function and f ◦ H (m) ≥ m.

4. Results
Let R be the set of all computable partial functions mapping N to N, and let θ : N → R
be a computable index (analogous to our other index function φ).
Let
B(n) = max θk (0)
k≤n

(4)

Lemma 2. Let f ∈ R be a total function. Then B(n) > f (n) infinitely often.
Proof: Suppose not. Then B(n) > f (n) only finitely many times, so there exists some
c ∈ N such that (∀n ∈ N ) : f (n) + c > B(n).
Let C(n, m) = f (n) + c. By a corollary of the Recursion Theorem, there exists
m ∈ N such that (∀n ∈ N) : θm (n) = C(m, n) = f (m) + c.
By definition, B(m) ≥ θm (0) = C(m, 0) = f (m) + c > B(m). So B(m) > B(m),
which is a contradiction.
n
Now suppose that at time n + 1, the agent has already taken actions y1 and made

observations xn , and is considering the expected utility of policy p. Let D = {s ∈ X N :
1
n
n
s1 = x1 }.
Theorem 1. If U is computably unbounded from above on D, then
n
n
E (U (xn Ψ (Q, p, y1 , xn )) |ΓQ (y1 ) = xn ) is either undefined or +∞.
1
1
1

Proof: Let UL : Dp → Z be as in definition 1. Then by Lemma 1, there exists H : Z →
Dp such that (∀m ∈ Z) : UL (H(m)) > m.
H here is intended to be used to construct sequences with high utility. Since H
outputs a string rather than a sequence, we will pad it to get a sequence. Let c ∈ X be

¯
¯
some arbitrary word in the perception alphabet. Then let H : Z → D, where H(n) is a
sequence beginning with H(n), followed by c, c, c, . . . .
n
For brevity, let Wp (q) = xn Ψ (q, p, y1 , xn ). Wp (q) represents the complete sequence
1
1
of perceptions received by the agent, assuming that it continues to implement policy p
in environment q.
We will now break up the expected utility into two terms, depending on whether or
not Q ∈ ΩC .

n
E (U (Wp (Q)) |ΓQ (y1 ) = xn )
1
n
= P (Q ∈ ΩC )E (U (Wp (Q)) |ΓQ (y1 ) = xn , Q ∈ ΩC )
1
n
+P (Q ∈ ΩC )E (U (Wp (Q)) |ΓQ (y1 ) = xn , Q ∈ ΩC )
/
/
1
n
U (Wp (q)) P ({q}|ΓQ (y1 ) = xn )
1

=
q∈ΩC

n
+P (Q ∈ ΩC )E (U (Wp (Q)) |ΓQ (y1 ) = xn , Q ∈ ΩC )
/
/
1

We will show that the series:
n
U (Wp (q)) P ({q}|ΓQ (y1 ) = xn )
1
q∈ΩC

has infinitely many terms ≥ 1. We will do this by finding a sequence of environments
whose utilities grows very quickly - more quickly than their probabilities can shrink.
By equation 4, for each j ∈ N there exists uj ∈ N such that uj ≤ j and θuj (0) =
B (j).
Now we define a map on function indices G : N → N such that:
φG(n) (γ) = H(θn (0))|γ|
So G takes the θ-index of an N → N function (say, g), and returns the φ-index of an
environment which is compatible with all the data so far, and which is guaranteed to
produce utility greater than g(0). We can assume that G is a computable function.
So our sequence of environments will be φG(uj )

∞
.
j=1

Then U Wp (φG(uj ) ) ≥ B(j). Now let
ρ(j) = max
¯
k≤j

1
ρ(G(k))

(5)

Then ρ is a computable, nondecreasing function. Since ρ is computable, B(j) ≥ ρ(j)
¯
¯
¯
1
1
infinitely often. Since uj ≤ j, then by definition, ρ(j) ≥ ρ(G(uj )) ≥ P ({φG(u ) }) .
¯
j

n
n
P (ΓQ (y1 ) = xn |Q = φG(uj ) ) = 1, so by Bayes’ Rule, P ({φG(uj ) }|ΓQ (y1 ) = xn ) ≥
1
1

P ({φG(uj ) }). Since both sides are positive, we take the reciprocal to get P ({φG(u 1) }|yn ,xn ) ≤
j

1
P ({φG(uj

. By transitivity, U Wp (φG(uj ) ) ≥
) })

1
n
P ({φG(uj ) }|ΓQ (y1 )=xn )
1

1

1

infinitely often, so

n
U Wp (φG(uj ) ) P ({φG(uj ) }|ΓQ (y1 ) = xn ) ≥ 1 infinitely often. Since the series con1

tains infinitely many terms ≥ 1, its limit is either +∞ or nonexistent.
Corollary 1. If U is computably unbounded from below on D, then
n
n
E (U (xn Ψ (Q, p, y1 , xn )) |ΓQ (y1 ) = xn ) is either undefined or −∞.
1
1
1

Proof: By definition, −U is computably unbounded from above. Thus, by theorem 1,
n
n
E (−U (xn Ψ (Q, p, y1 , xn )) |ΓQ (y1 ) = xn ) is either undefined or +∞. So
1
1
1
n
n
n
n
n
n
E (U (x1 Ψ (Q, p, y1 , x1 )) |ΓQ (y1 )) = −E (−U (xn Ψ (Q, p, y1 , xn )) |ΓQ (y1 ) = xn )
1
1
1

is either undefined or −∞.
Corollary 2. If U is computably unbounded from both below and above on D, then
n
n
E (U (xn Ψ (Q, p, y1 , xn )) |ΓQ (y1 ) = xn ) is undefined.
1
1
1
n
n
Proof: By theorem 1, E (U (xn Ψ (Q, p, y1 , xn )) |ΓQ (y1 ) = xn ) is either undefined or
1
1
1

+∞. By corollary 1, it is either undefined or −∞. Thus it is undefined.

5. Discussion
Our main result implies that if you have an unbounded, perception determined, computable utility function, and you use a Solomonoﬀ-like prior (Solomonoﬀ 1964), then
you have no way to choose between policies using expected utility. So which of these
things should we change?
We could use a non-perception determined utility function. Then our main result
would not apply. In this case, the existence of bounded expected utility will depend on
the utility function. It may be possible to generalize our argument to some larger class
of utility functions which have a diﬀerent domain.
We could use an uncomputable utility function. For instance, if the utility of any
perception-sequence is defined as equal to its Kolmogorov complexity, then the utility
function is unbounded but the expected utility of any policy is finite.
We could use a smaller hypothesis space; perhaps not all computable environments
should be considered.
The simplest approach may be to use a bounded utility function. Then convergence
is guaranteed.

References
Hutter, Marcus. 2007. “Universal Algorithmic Intelligence: A Mathematical Top→Down Approach.”
In Artificial General Intelligence, edited by Ben Goertzel and Cassio Pennachin, 227–290. Cognitive
Technologies. Berlin: Springer. doi:10.1007/978-3-540-68677-4_8.
Solomonoﬀ, Ray J. 1964. “A Formal Theory of Inductive Inference. Part I.” Information and Control 7
(1): 1–22. doi:10.1016/S0019-9958(64)90223-2.

